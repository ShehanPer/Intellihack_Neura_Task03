{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-tuning Challenge: Enhancing Qwen 2.5 3B for AI Research QA\n",
    "\n",
    "This notebook delivers documentation for the task from start to end which was stored in several notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings Vector for Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Major Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain pypdf markdown numpy\n",
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Loading Dataset\n",
    "\n",
    "In this cell, we are reading and loading the dataset from PDF and Markdown files. We use the `glob` library to find all files with `.pdf` and `.md` extensions in the `rawdata` directory. \n",
    "\n",
    "#### Libraries Used:\n",
    "- `langchain.document_loaders.PyPDFLoader`: This class is used to load PDF documents. It reads the content of PDF files and converts them into a format that can be processed further.\n",
    "- `langchain.document_loaders.TextLoader`: This class is used to load text documents, including Markdown files. It reads the content of text files and converts them into a format that can be processed further.\n",
    "- `glob`: This module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell.\n",
    "\n",
    "#### Process:\n",
    "1. **Reading PDF Files**:\n",
    "   - We use `glob.glob(\"rawdata/*.pdf\")` to get a list of all PDF files in the `rawdata` directory.\n",
    "   - For each PDF file, we create an instance of `PyPDFLoader` and load the documents using the `load()` method.\n",
    "   - The loaded documents are then added to the `raw_data` list.\n",
    "\n",
    "2. **Reading Markdown Files**:\n",
    "   - Similarly, we use `glob.glob(\"rawdata/*.md\")` to get a list of all Markdown files in the `rawdata` directory.\n",
    "   - For each Markdown file, we create an instance of `TextLoader` with UTF-8 encoding and load the documents using the `load()` method.\n",
    "   - The loaded documents are then added to the `raw_data` list.\n",
    "\n",
    "By the end of this cell, the `raw_data` list contains all the documents from the specified PDF and Markdown files, ready for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "import glob\n",
    "\n",
    "#### Reading Dataset\n",
    "\n",
    "pdf_files = glob.glob(\"rawdata/*.pdf\")\n",
    "md_files = glob.glob(\"rawdata/*.md\")\n",
    "raw_data=[]\n",
    "for file in pdf_files:     # Load a PDF files\n",
    "  pdf_loader = PyPDFLoader(file)\n",
    "  pdf_documents = pdf_loader.load()\n",
    "  raw_data.extend(pdf_documents)\n",
    "for file in md_files:# Load a Markdown file\n",
    "  md_loader = TextLoader(file, encoding=\"utf-8\")\n",
    "  md_documents = md_loader.load()\n",
    "  raw_data.extend(md_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in raw_data[0:5]:  #checking content of first 5 documents\n",
    "  print(data.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Documents into Chunks\n",
    "\n",
    "Here, we are splitting the loaded documents into smaller chunks to facilitate better processing and embedding generation. We use the `RecursiveCharacterTextSplitter` from the `langchain.text_splitter` module to achieve this.\n",
    "\n",
    "#### Libraries Used:\n",
    "- `langchain.text_splitter.RecursiveCharacterTextSplitter`: This class is used to split text documents into smaller chunks. It ensures that the chunks are of a manageable size and maintains context by allowing some overlap between chunks.\n",
    "\n",
    "#### Process:\n",
    "1. **Initialize Text Splitter**:\n",
    "   - We create an instance of `RecursiveCharacterTextSplitter` with the following parameters:\n",
    "     - `chunk_size=400`: This sets the maximum size of each chunk to 400 characters.\n",
    "     - `chunk_overlap=60`: This sets the overlap between consecutive chunks to 60 characters to maintain context.\n",
    "\n",
    "2. **Split Documents**:\n",
    "   - We use the `split_documents()` method of the `text_splitter` instance to split the `raw_data` documents into smaller chunks.\n",
    "   - The resulting chunks are stored in the `document_chunks` list.\n",
    "\n",
    "3. **Print Sample Chunks**:\n",
    "   - We print the content and metadata of the first 5 chunks to verify the splitting process.\n",
    "\n",
    "By the end of this cell, the `document_chunks` list contains the smaller chunks of the original documents, ready for further processing and embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,  # Maximum size of each chunk\n",
    "    chunk_overlap=60  # Overlap between chunks to maintain context\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "document_chunks = text_splitter.split_documents(raw_data)\n",
    "\n",
    "\n",
    "for chunk in document_chunks[:5]:  # Print the first 5 chunks\n",
    "    print(\"Chunk Content:\", chunk.page_content)\n",
    "    print(\"Metadata:\", chunk.metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vector Embeddings\n",
    "\n",
    "In this cell, we are generating vector embeddings for the document chunks using a pre-trained model from the `sentence_transformers` library. These embeddings will be used for various downstream tasks such as similarity search, clustering, and more.\n",
    "\n",
    "#### Libraries Used:\n",
    "- `sentence_transformers.SentenceTransformer`: This class is used to load pre-trained models and generate embeddings for text data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process:\n",
    "1. **Load Embedding Model**:\n",
    "   - We load the `multi-qa-mpnet-base-dot-v1` model using the `SentenceTransformer` class. This model is specifically designed for generating high-quality embeddings for question-answering tasks and general-purpose semantic search.\n",
    "   - **Reason for Choosing `multi-qa-mpnet-base-dot-v1`**:\n",
    "     - **Performance**: This model is known for its high performance in generating embeddings that capture semantic meaning effectively. It has larger dimensions compared to other available models like `multi-qa-MiniLM-L6-cos-v1`, `all-MiniLM-L6-v2`\n",
    "     - **Versatility**: It is optimized for question-answering, which aligns with the goal of creating a chatbot that answers AI-related questions.\n",
    "\n",
    "\n",
    "2. **Generate Embeddings**:\n",
    "   - We extract the text content from each chunk in `document_chunks` and store them in the `chunk_texts` list.\n",
    "   - We use the `encode()` method of the `embedding_model` to generate embeddings for the `chunk_texts`.\n",
    "   - The resulting embeddings are stored in the `embeddings` list.\n",
    "\n",
    "3. **Print Sample Embedding**:\n",
    "   - We print the embedding for the first chunk to verify the embedding generation process.\n",
    "\n",
    "By the end of this cell, the `embeddings` list contains the vector embeddings for each document chunk, ready for use in further analysis and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Create vector Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load an embedding model\n",
    "embedding_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "chunk_texts = [chunk.page_content for chunk in document_chunks]\n",
    "embeddings = embedding_model.encode(chunk_texts)\n",
    "\n",
    "# Example: Print the embedding for the first chunk\n",
    "print(\"First Chunk Embedding:\", embeddings[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing and Converting Embeddings to NumPy Array\n",
    "\n",
    "In this cell, we are converting the generated embeddings into a NumPy array and normalizing them. This step ensures that the embeddings are in a consistent format and have unit length, which is important for many machine learning algorithms and similarity calculations.\n",
    "\n",
    "#### Libraries Used:\n",
    "- `numpy`: This library is used for numerical operations in Python. It provides support for arrays and matrices, along with a collection of mathematical functions to operate on these data structures.\n",
    "\n",
    "#### Process:\n",
    "1. **Convert to NumPy Array**:\n",
    "   - We convert the list of embeddings into a NumPy array using `np.array(embeddings)`. This allows us to leverage NumPy's efficient numerical operations.\n",
    "   - We specify the data type as `float32` to ensure that the embeddings are stored in a format that is compatible with most machine learning frameworks.\n",
    "\n",
    "2. **Normalize Embeddings**:\n",
    "   - We normalize the embeddings to have unit length using `np.linalg.norm()`. This is done by dividing each embedding vector by its L2 norm.\n",
    "   - Normalization ensures that the embeddings have a consistent scale, which is important for similarity calculations and other downstream tasks.\n",
    "\n",
    "\n",
    "`embeddings_array` contains the normalized embeddings in a NumPy array format, which will be used in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Convert embeddings to a NumPy array\n",
    "embeddings_array = np.array(embeddings).astype('float32')\n",
    "embeddings_array /= np.linalg.norm(embeddings_array, axis=1, keepdims=True)\n",
    "\n",
    "print(embeddings_array.shape) #checking the vector store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following is sample function which returns embeddings for a user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector(chunk):\n",
    "    '''This function generates an vector for a given chunk/query\n",
    "    Make sure you have imported sentence_transformers and defined the embedding_model \n",
    "    For this task we will use the multi-qa-mpnet-base-dot-v1 model'''\n",
    "    embeddings = embedding_model.encode(chunk_texts)\n",
    "    query_embedding = embedding_model.encode([query_text]).astype('float32')\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)     # Normalize query embedding \n",
    "    return query_embedding\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Chunks and Embeddings\n",
    "\n",
    "We save the document chunks and their corresponding embeddings to files for later access.\n",
    "\n",
    "#### Libraries Used:\n",
    "- `pickle`: This module is used for serializing and deserializing Python objects.\n",
    "- `numpy`: This library is used for numerical operations and saving arrays.\n",
    "\n",
    "#### Process:\n",
    "   - We use `pickle.dump()` to save the `document_chunks` list to a binary file named `chunks.pkl`.\n",
    "   - We use `np.save()` to save the `embeddings_array` to a file named `vector_store.npy`.\n",
    "\n",
    "the document chunks and embeddings are saved to disk, allowing for easy retrieval and reuse in future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Saving chunks for later access\n",
    "import pickle\n",
    "\n",
    "# Save chunks to a binary file\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(document_chunks, f)\n",
    "\n",
    "np.save(\"vector_store.npy\", embeddings_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Dataset for Model Training\n",
    "\n",
    "we are generating a dataset to train the model using the pre-trained `Qwen2.5-3B model`. This approach is chosen due to resource(Unavailability of free APIs for better models) and time constraints. \n",
    "\n",
    "you will be skipping initial steps as we already have our data loaded.\n",
    "\n",
    "### Overview\n",
    "- **Objective**: Create a dataset suitable for training the chatbot model that answers AI-related questions.\n",
    "- **Approach**: Utilize the `Qwen2.5-3B model` to generate the dataset while using the`multi-qa-mpnet-base-dot-v1` model to generate embeddings efficiently.\n",
    "\n",
    "### Steps:\n",
    "1. **Load Data**: Loading the vector embeddings previously generated.\n",
    "2. **Generate dataset**: leveraging `faiss` to get similar embeddings. feed them to model using predefined prompts for generating data.\n",
    "3. **Save Dataset**: Store the generated dataset for use in model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Similar Embeddings with FAISS\n",
    "\n",
    "we use FAISS to find similar embeddings for interactions.\n",
    "\n",
    "#### Libraries Used:\n",
    "- `faiss`: A library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "#### Process:\n",
    "1. **Normalize Embeddings**:\n",
    "   - Ensure embeddings are normalized for cosine similarity.\n",
    "\n",
    "2. **Create FAISS Index**:\n",
    "   - Create a FAISS index for inner product (cosine similarity) using the embeddings.\n",
    "\n",
    "3. **Define Functions**:\n",
    "   - `get_embeddings(query_text)`: Encodes a query text, normalizes it, and finds the top 3 similar document chunks.\n",
    "   - `get_similar_indices(chunk_index, k)`: Finds the top `k` similar document chunks for a given chunk index, excluding itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "\n",
    "# Create a FAISS index for cosine similarity (using Inner Product)\n",
    "dimension = embeddings_array.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(embeddings_array)\n",
    "\n",
    "def get_embeddings(query_text=\"\"):\n",
    "  query_embedding = embedding_model.encode([query_text]).astype('float32')\n",
    "  # Normalize query embedding before searching\n",
    "  query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "  distances, indices = index.search(query_embedding, k=3)\n",
    "  related_indices = [i for i in indices[0] if i!=-1]\n",
    "  return [document_chunks[i].page_content for i in related_indices]\n",
    "\n",
    "\n",
    "def get_similar_indices(chunk_index=-1,k=3):\n",
    "    chunk_embedding = embeddings_array[chunk_index].reshape(1, -1)  # Ensure correct shape\n",
    "\n",
    "    # Perform FAISS search\n",
    "    distances, indices = index.search(chunk_embedding, k=k+1)  # k+1 to exclude itself\n",
    "\n",
    "    # Remove the chunk itself from results (if present)\n",
    "    related_indices = [i for i in indices[0] if (i != chunk_index and i!=-1)]\n",
    "\n",
    "    # Return related chunks\n",
    "    return [document_chunks[i].page_content for i in related_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the retrieval of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the vector database\n",
    "query_text = \"What are the first-generation reasoning models?\"\n",
    "retrieved_chunks= get_embeddings(query_text)\n",
    "print(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates for Dataset Generation\n",
    "\n",
    "We define various prompt templates used to generate responses and queries for the dataset.\n",
    "We are doing query generation+query evolution along with Expected response generation for optimal dataset Creation \n",
    "\n",
    "\n",
    "#### Functions:\n",
    "1. **`Queryprompt(contexts)`**:\n",
    "   - Generates a prompt to create JSON objects with questions or statements related to the given contexts.\n",
    "\n",
    "2. **`multi_context_template(context, original_input)`**:\n",
    "   - Rewrites an input to require information from all elements in the context, ensuring conciseness and relevance.\n",
    "\n",
    "3. **`reasoning_template(context, original_input)`**:\n",
    "   - Rewrites an input to explicitly request multi-step reasoning, ensuring it is concise and understandable.\n",
    "\n",
    "4. **`hypothetical_scenario_template(context, original_input)`**:\n",
    "   - Rewrites an input to incorporate a hypothetical scenario, encouraging application of knowledge from the context.\n",
    "\n",
    "5. **`response_template(context, input)`**:\n",
    "   - Generates a detailed and informative response to the given input, using relevant details from the context.\n",
    "\n",
    "These templates help in structuring the prompts for generating diverse and relevant dataset entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Queryprompt(contexts):    \n",
    "   return f\"\"\"I want you act as a copywriter. Based on the given context,\n",
    "which is list of strings, Generate a list of JSON objects\n",
    "with a `input` key. The `input` can be a question or a statement\n",
    " related to the contexts .\n",
    "\n",
    "contexts:\n",
    "{contexts}\"\"\"\n",
    "\n",
    "\n",
    "def multi_context_template(context,original_input):\n",
    "  return f\"\"\"\n",
    "  I want you to rewrite the given `input` so that it requires readers to use information from all elements in `Context`.\n",
    "\n",
    "1. `Input` should require information from all `Context` elements.\n",
    "2. `Rewritten Input` must be concise and fully answerable from `Context`.\n",
    "3. Do not use phrases like 'based on the provided context.'\n",
    "4. `Rewritten Input` should not exceed 15 words.\n",
    "\n",
    "Context: {context}\n",
    "Input: {original_input}\n",
    "Rewritten Input:\n",
    "\"\"\"\n",
    "\n",
    "def reasoning_template(context,original_input):\n",
    "  return f\"\"\"\n",
    "I want you to rewrite the given `input` so that it explicitly requests multi-step reasoning.\n",
    "\n",
    "1. `Rewritten Input` should require multiple logical connections or inferences.\n",
    "2. `Rewritten Input` should be concise and understandable.\n",
    "3. Do not use phrases like 'based on the provided context.'\n",
    "4. `Rewritten Input` must be fully answerable from `Context`.\n",
    "5. `Rewritten Input` should not exceed 15 words.\n",
    "\n",
    "Context: {context}\n",
    "Input: {original_input}\n",
    "Rewritten Input:\n",
    "\"\"\"\n",
    "\n",
    "def hypothetical_scenario_template(context,original_input):\n",
    "  return f\"\"\"\n",
    "I want you to rewrite the given `input` to incorporate a hypothetical or speculative scenario.\n",
    "\n",
    "1. `Rewritten Input` should encourage applying knowledge from `Context` to deduce outcomes.\n",
    "2. `Rewritten Input` should be concise and understandable.\n",
    "3. Do not use phrases like 'based on the provided context.'\n",
    "4. `Rewritten Input` must be fully answerable from `Context`.\n",
    "5. `Rewritten Input` should not exceed 15 words.\n",
    "\n",
    "Context: {context}\n",
    "Input: {original_input}\n",
    "Rewritten Input:\n",
    "\"\"\"\n",
    "\n",
    "def response_template(context, input):\n",
    "  return f\"\"\"\n",
    "I want you to generate a well-structured response to the given `input`, using relevant details from `Context`.\n",
    "1. `Response` must be detailed, informative.\n",
    "2. `Response` should provide clear and accurate information without unnecessary repetition.\n",
    "3. Do not use phrases like 'based on the provided context.'\n",
    "4. Maintain a professional and engaging tone.\n",
    "\n",
    "Context: {context}\n",
    "Input: {input}\n",
    "Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response of the model contains input and some other unnecessary information for dataset. So we process the responses with following functions in different cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_questions(response):\n",
    "    chatbot_response = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "    cleaned_response = re.sub(r\"```json|```\", \"\", chatbot_response).strip()\n",
    "\n",
    "    # Try to find the JSON structure using regex\n",
    "    questions = re.findall(r'\"input\":\\s*\"([^\"]+)\"', cleaned_response)\n",
    "\n",
    "    return questions\n",
    "\n",
    "def extract_response(response):\n",
    "    text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "    start_index = text.find('Response:')\n",
    "\n",
    "    if start_index != -1:\n",
    "        # Slice the text from the 'Rewritten Input:' point onwards\n",
    "        extracted_text = text[start_index+9:].strip()\n",
    "        return extracted_text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_rewritten_input(response):\n",
    "    # Pattern to extract text between \"Rewritten Input:\" and \"<|endoftext|>\"\n",
    "    text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "    start_index = text.find('Rewritten Input:')\n",
    "\n",
    "    if start_index != -1:\n",
    "        # Slice the text from the 'Rewritten Input:' point onwards\n",
    "        extracted_text = text[start_index+16:].strip()\n",
    "        return extracted_text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolving Queries and Generating Responses\n",
    "\n",
    "In this cell, we define a function to perform random evolution steps on queries and generate responses, creating entries for the training dataset.\n",
    "\n",
    "#### Libraries Used:\n",
    "- `transformers.TextStreamer`: Used for streaming text generation.\n",
    "- `re`: Regular expression operations.\n",
    "\n",
    "#### Process:\n",
    "1. **Templates and Evolution Steps**:\n",
    "   - Define a list of evolution templates (`multi_context_template`, `reasoning_template`, `hypothetical_scenario_template`).\n",
    "   - Set the number of evolution steps to apply.\n",
    "\n",
    "2. **Function `evolve_query(original_input, context)`**:\n",
    "   - Randomly choose a template from `evolution_templates`.\n",
    "   - Generate an evolved prompt using the chosen template.\n",
    "   - Tokenize and generate a response using the model.\n",
    "   - Extract the rewritten input from the response.\n",
    "   - Generate a final response using the `response_template`.\n",
    "   - Append the original input, final response, and context to the `training_dataset`.\n",
    "\n",
    "`training_dataset` contains evolved queries and their corresponding responses, ready for use in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers\n",
    "!pip install tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "import re\n",
    "\n",
    "evolution_templates = [multi_context_template, reasoning_template, hypothetical_scenario_template]\n",
    "\n",
    "# Number of evolution steps to apply\n",
    "num_evolution_steps = 3\n",
    "\n",
    "\n",
    "training_dataset=[]\n",
    "\n",
    "\n",
    "# Function to perform random evolution steps\n",
    "def evolve_query(original_input, context):\n",
    "      # Choose a random (or using custom logic) template from the list\n",
    "      chosen_template = random.choice(evolution_templates)\n",
    "      # Replace the placeholders with the current context and input\n",
    "      evolved_prompt = chosen_template(str(context), original_input)\n",
    "      # Update the current input with the \"Rewritten Input\" section\n",
    "\n",
    "      inputs = tokenizer(evolved_prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "      text_streamer = TextStreamer(tokenizer)\n",
    "      response = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)\n",
    "      evolved_question = extract_rewritten_input(response)\n",
    "\n",
    "      prompt_for_response = response_template(str(context), evolved_question)\n",
    "\n",
    "      inputs = tokenizer(prompt_for_response, return_tensors = \"pt\").to(\"cuda\")\n",
    "      text_streamer = TextStreamer(tokenizer)\n",
    "      response = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)\n",
    "      final_response = extract_response(response)\n",
    "\n",
    "      training_dataset.append({\"Input\": original_input, \"Response\": final_response, \"Context\": context})\n",
    "      original_input = evolved_question\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating training dataset\n",
    "\n",
    "So far we were laying the groundwork for dataset generation. Now we will start the process by loading the required libraries and our base model\n",
    "#### Libraries Used:\n",
    "- `torch`: PyTorch library for tensor operations and deep learning.\n",
    "- `transformers`: Hugging Face library for transformer models.\n",
    "- `numpy`: Library for numerical operations.\n",
    "- `datasets`: Hugging Face library for loading and processing datasets.\n",
    "- `sklearn.model_selection`: Provides tools for splitting datasets into training and testing sets.\n",
    "- `unsloth.FastLanguageModel`: Custom library for efficient language model operations.\n",
    "\n",
    "By importing these libraries, we set up the environment for model loading, data processing, and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Configuring the Model\n",
    "\n",
    "Here we load and configure the Qwen2.5-3B model using the `FastLanguageModel` class with specific settings to optimize performance and memory usage.\n",
    "\n",
    "#### Parameters:\n",
    "- **`max_seq_length`**: Set to 2048 to define the maximum sequence length.\n",
    "- **`dtype`**: Automatically detected or set to `float16` for certain GPUs.\n",
    "- **`load_in_4bit`**: Enabled to use 4-bit quantization, reducing memory usage.\n",
    "\n",
    "#### Process:\n",
    "1. **Model Selection**:\n",
    "   - Choose from a list of available models, including various versions of Qwen2.5 and other models from the `unsloth` repository.\n",
    "\n",
    "2. **Load Model**:\n",
    "   - Use `FastLanguageModel.from_pretrained()` to load the selected model (`unsloth/Qwen2.5-3B`), with specified parameters for sequence length, data type, and quantization.\n",
    "\n",
    "3. **Configure PEFT (Parameter-Efficient Fine-Tuning)**:\n",
    "   - Apply PEFT to the model using `FastLanguageModel.get_peft_model()` with specified parameters for target modules, LoRA settings, and gradient checkpointing.\n",
    "\n",
    "4. **Enable Faster Inference**:\n",
    "   - Call `FastLanguageModel.for_inference()` to enable native 2x faster inference.\n",
    "\n",
    "By the end of this cell, the Qwen2.5-3B model is loaded and configured for efficient training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # Can select any from the below:\n",
    "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
    "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
    "    # And also all Instruct versions and Math. Coding verisons!\n",
    "    model_name = \"unsloth/Qwen2.5-3B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again due time contraints and colab environment limitations we skipped evolution of queries with this function for faster dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def without_evolve(original_input, context):\n",
    "      prompt_for_response = response_template(str(context), original_input)\n",
    "      inputs = tokenizer(prompt_for_response, return_tensors = \"pt\").to(\"cuda\")\n",
    "      text_streamer = TextStreamer(tokenizer)\n",
    "      response = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)\n",
    "      final_response = extract_response(response)\n",
    "\n",
    "      training_dataset.append({\"Input\": original_input, \"Response\": final_response, \"Context\": context})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Queries and Saving Dataset\n",
    "\n",
    "In this cell, we generate queries based on document chunks, process them, and save the resulting dataset.\n",
    "\n",
    "#### Process:\n",
    "1. **Generate Queries**:\n",
    "   - Loop 800 times to generate queries.\n",
    "   - Randomly select a reference index from `document_chunks`.\n",
    "   - Retrieve similar contexts using `get_similar_indices(reference_index)`.\n",
    "   - Generate a prompt using `Queryprompt(contexts)`.\n",
    "   - Tokenize the prompt and generate a response using the model.\n",
    "   - Extract questions from the response.\n",
    "   - If questions are found, randomly select one as `test_question`.\n",
    "\n",
    "2. **Process Query**:\n",
    "   - Call `without_evolve(test_question, contexts)` to process the query without evolution (or use `evolve_query` if needed).\n",
    "\n",
    "3. **Save Dataset**:\n",
    "   - Save the `training_dataset` to a JSON file named `dataset.json`.\n",
    "\n",
    "the generated queries and their corresponding contexts are processed and saved in a JSON file for use in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(800):\n",
    "  reference_index = random.randint(0, len(document_chunks) - 1)\n",
    "  contexts = get_similar_indices(reference_index)\n",
    "  prompt = Queryprompt(contexts)\n",
    "  inputs = tokenizer(prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "  text_streamer = TextStreamer(tokenizer)\n",
    "  response = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 150)\n",
    "  questions= extract_questions(response)\n",
    "  if (len(questions)!=0):\n",
    "    test_question = random.choice(questions)\n",
    "  else :\n",
    "    continue\n",
    "  #evolve_query(test_question,contexts)     \n",
    "  without_evolve(test_question,contexts)\n",
    "\n",
    "  with open('dataset.json', 'w') as json_file:\n",
    "      json.dump(training_dataset, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We will be skipping some steps. refer model_training.ipynb for more details\n",
    "\n",
    "### Overview\n",
    "- **Objective**: Train the Qwen2.5-3B model to answer AI-related questions using the generated dataset.\n",
    "- **Approach**: Load the dataset, configure the training parameters, and train the model using the `SFTTrainer` from the `trl` library.\n",
    "\n",
    "### Steps:\n",
    "1. **Preprocess Data**: Tokenize the data and prepare it for training.\n",
    "2. **Configure Training**: Set up the training arguments and configure the model for training.\n",
    "3. **Train Model**: Use the `SFTTrainer` to train the model on the dataset.\n",
    "\n",
    "By following these steps, we will fine-tune the Qwen2.5-3B model to improve its ability to answer AI-related questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Pre-trained Model\n",
    "\n",
    "We have already loaded our model for dataset creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Formatting the Dataset\n",
    "\n",
    "Here we will split our dataset into training and testing sets, and format the prompts for training.\n",
    "\n",
    "#### Process:\n",
    "1. **Load Dataset**:\n",
    "   - Load the dataset from the JSON file using `load_dataset(\"json\", data_files=\"/content/dataset.json\")`.\n",
    "   - Split the dataset into training and testing sets with a 99:1 ratio using `train_test_split`.\n",
    "\n",
    "2. **Define Prompt Template**:\n",
    "   - Define the `alpaca_prompt` template to structure the instruction, input, and response for training.\n",
    "   - The Alpaca format is used here to provide a clear and consistent structure for the model to learn from. It includes:\n",
    "     - **Instruction**: Describes the task to be performed.\n",
    "     - **Input**: Provides context or additional information needed to complete the task.\n",
    "     - **Response**: The expected output or answer to the instruction based on the input.\n",
    "\n",
    "3. **Format Prompts**:\n",
    "   - Define the `formatting_prompts_func` function to format the dataset examples using the `alpaca_prompt` template.\n",
    "   - Add the end-of-sequence token (`EOS_TOKEN`) to each formatted text to indicate the end of the response.\n",
    "\n",
    "4. **Apply Formatting**:\n",
    "   - Apply the `formatting_prompts_func` to the training dataset using the `map` function.\n",
    "\n",
    "#### Reason for using Alpaca Format:\n",
    "- **Consistency**: The Alpaca format provides a consistent structure for each training example, making it easier for the model to learn the relationship between instructions, inputs, and responses.\n",
    "- **Clarity**: By clearly separating the instruction, input, and response, the model can better understand the context and generate appropriate answers.\n",
    "- **Flexibility**: This format can accommodate a wide range of tasks and contexts, making it suitable for training a versatile chatbot model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "# Load the dataset from the JSON file\n",
    "dataset = load_dataset(\"json\", data_files=\"/content/dataset.json\")\n",
    "dataset = dataset['train'].train_test_split(test_size=0.01, shuffle=True)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"Input\"]\n",
    "    inputs = examples[\"Context\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "#dataset = load_dataset(\"json\", data_files=\"/content/output.json\")\n",
    "#dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Concatenate the datasets using concatenate_datasets\n",
    "#dataset = concatenate_datasets([dataset['train'], train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['train'][302])  # Access the 'train' split and then the element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring and Initializing the Trainer\n",
    "\n",
    "Now we configure and initialize the `SFTTrainer` from the `trl` library to train the model using the formatted dataset.\n",
    "\n",
    "#### Parameters and Reasoning:\n",
    "\n",
    "1. **Trainer Initialization**:\n",
    "   - **`model`**: The pre-trained Qwen2.5-3B model loaded earlier.\n",
    "   - **`tokenizer`**: The tokenizer associated with the model.\n",
    "   - **`train_dataset`**: The training dataset prepared and formatted in previous steps.\n",
    "   - **`dataset_text_field`**: `\"text\"` - The field in the dataset containing the formatted text.\n",
    "   - **`max_seq_length`**: `2048` - Maximum sequence length for the model.\n",
    "   - **`dataset_num_proc`**: `2` - Number of processes to use for data loading.\n",
    "\n",
    "2. **Training Arguments**:\n",
    "   - **`per_device_train_batch_size`**: `2` - Small batch size due to limited dataset size and to fit within memory constraints.\n",
    "   - **`gradient_accumulation_steps`**: `4` - Accumulate gradients over 4 steps to effectively increase the batch size without requiring more memory.\n",
    "   - **`warmup_steps`**: `5` - Number of warmup steps to gradually increase the learning rate at the start of training.\n",
    "   - **`max_steps`**: `60` - Total number of training steps, kept low due to the small dataset size.\n",
    "   - **`learning_rate`**: `2e-4` - Learning rate for the optimizer, chosen to balance between convergence speed and stability.\n",
    "   - **`optim`**: `\"adamw_8bit\"` - Use the AdamW optimizer with 8-bit precision to reduce memory usage.\n",
    "   - **`weight_decay`**: `0.01` - Weight decay for regularization to prevent overfitting.\n",
    "   - **`lr_scheduler_type`**: `\"linear\"` - Linear learning rate scheduler for gradual learning rate decay.\n",
    "   - **`seed`**: `3407` - Seed for reproducibility.\n",
    "   - **`output_dir`**: `\"outputs\"` - Directory to save training outputs and checkpoints.\n",
    "   - **`report_to`**: `\"none\"` - Disable reporting to external services (e.g., WandB).\n",
    "\n",
    "the `SFTTrainer` is configured and initialized, ready to train the model on the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",  # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Trained Model\n",
    "\n",
    "\n",
    "Here we test out newly trained model for sample instructions manually\n",
    "\n",
    "By the end of this cell, the model generates a response to the given instruction using the specified prompt format, demonstrating its ability to provide concise and relevant answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a brief and precise response that directly answers the question without unnecessary information.\n",
    "\n",
    "1. Keep the response brief and concise.\n",
    "2. Avoid elaborating on unrelated topics.\n",
    "3. Provide only the most relevant information from the input.\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "   query_alpaca_prompt.format(\n",
    "        \"is deepseek R1 Zero fast? \", # instruction\n",
    "        \" \", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Trained Model Using 4 bit Quantizising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 4-bit Q4_K_M GGUF\n",
    "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Loading and Using Embeddings for Similarity Search (RAG Retrieval)\n",
    "\n",
    "In this cell, we load embeddings and document chunks, and set up a FAISS index for similarity search.\n",
    "\n",
    "#### Process:\n",
    "1. **Load Embedding Model**:\n",
    "   - Load the `multi-qa-mpnet-base-dot-v1` model from `SentenceTransformer` for generating embeddings.\n",
    "\n",
    "2. **Load Document Chunks**:\n",
    "   - Load the document chunks from a pickle file (`chunks.pkl`).\n",
    "   - Convert the `LangChain` `Document` objects to a list of dictionaries containing text and metadata.\n",
    "\n",
    "3. **Load Embeddings**:\n",
    "   - Load the precomputed embeddings from a NumPy file (`vector_store.npy`).\n",
    "   - Print the shape of the loaded embeddings to verify.\n",
    "\n",
    "4. **Normalize Embeddings**:\n",
    "   - Normalize the embeddings to ensure they are suitable for cosine similarity calculations.\n",
    "\n",
    "5. **Create FAISS Index**:\n",
    "   - Create a FAISS index for inner product (cosine similarity) using the loaded embeddings.\n",
    "   - Add the embeddings to the FAISS index.\n",
    "\n",
    "6. **Define Function for Similarity Search**:\n",
    "   - `get_embeddings(query_text)`: Encodes a query text, normalizes it, and finds the top 3 similar document chunks using the FAISS index.\n",
    "   - Returns the text content of the related document chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "\n",
    "\n",
    "with open(\"chunks.pkl\", \"rb\") as f:     #loading chunks\n",
    "    document_chunks = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Convert LangChain Document objects to dict\n",
    "chunk_data = [{\"text\": doc.page_content, \"metadata\": doc.metadata} for doc in document_chunks]\n",
    "\n",
    "\n",
    "embeddings_array = np.load(\"vector_store.npy\")\n",
    "\n",
    "# Check loaded data\n",
    "#print(pick_chunks[0].page_content)  # Print first chunk text\n",
    "print(embeddings_array.shape)\n",
    "\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "\n",
    "# Create a FAISS index for cosine similarity (using Inner Product)\n",
    "dimension = embeddings_array.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(embeddings_array)\n",
    "\n",
    "def get_embeddings(query_text=\"\"):\n",
    "  query_embedding = embedding_model.encode([query_text]).astype('float32')\n",
    "  # Normalize query embedding before searching\n",
    "  query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "  distances, indices = index.search(query_embedding, k=3)\n",
    "  related_indices = [i for i in indices[0] if i!=-1]\n",
    "  return [document_chunks[i].page_content for i in related_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~Add LSTM for trained model~~\n",
    "\n",
    "This part is not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMMemory(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(LSTMMemory, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, input_sequence, hidden_state=None):\n",
    "        output, (hn, cn) = self.lstm(input_sequence, hidden_state)\n",
    "        return output, (hn, cn)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state and cell state for LSTM\n",
    "        return (torch.zeros(1, batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, batch_size, self.hidden_dim))\n",
    "\n",
    "# Initialize the LSTM Memory\n",
    "input_dim = 768  # Embedding dimension (based on your model)\n",
    "hidden_dim = 512\n",
    "lstm_memory = LSTMMemory(input_dim, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response generate\n",
    "Response generating prompt and procedure is defined here. Chat History integration in not integrated yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "\n",
    "# Generate Response Function\n",
    "def generate_response(query, chat_history, model, tokenizer, lstm_memory):\n",
    "    # Retrieve relevant chunks\n",
    "    relevant_chunks = get_embeddings(query)\n",
    "    print(f\"relevant chunks: \\n{relevant_chunks}\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    context = ' '.join(chat_history + relevant_chunks)\n",
    "    context=relevant_chunks\n",
    "    prompt= f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a brief and precise response that directly answers the question without unnecessary information.\n",
    "\n",
    "1. Keep the response brief and concise.\n",
    "2. Avoid elaborating on unrelated topics.\n",
    "3. Provide only the most relevant information from the input.\n",
    "### Instruction:\n",
    "{query}\n",
    "\n",
    "### Input:\n",
    "{context}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    # Prepare input for LSTM\n",
    "  #  inputs = tokenizer(context, return_tensors='pt')\n",
    "  #  embeddings = model(**inputs).last_hidden_state.mean(dim=1).unsqueeze(0)\n",
    "\n",
    "    # Update LSTM memory\n",
    " #   lstm_output, hidden_state = lstm_memory(embeddings, hidden_state)\n",
    "\n",
    "    # Generate response\n",
    "#    input_ids = tokenizer.encode(query + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "#    response_ids = model.generate(input_ids, max_length=100)\n",
    "#    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "    inputs= tokenizer(\n",
    "      [\n",
    "        prompt# output - leave this blank for generation!\n",
    "      ], return_tensors = \"pt\").to(\"cuda\")\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    response = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 200)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Chat Bot\n",
    "\n",
    "You can keep chatting with newly trained model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Chat Loop\n",
    "print(\"Start chatting with your bot (type 'exit' to stop):\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    response = generate_response(\n",
    "        user_input, chat_history, model, tokenizer, lstm_memory\n",
    "    )\n",
    "    response = tokenizer.decode(response[0],skip_special_tokens=True)\n",
    "\n",
    "    # Update chat history for context and memory\n",
    "    chat_history.append(user_input)\n",
    "    chat_history.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Journey**  \n",
    "Our journey in developing and training a custom AI model involved multiple challenges, adaptations, and optimizations. Initially, we attempted to train the model on our local machines but encountered significant **dependency issues**, making it impractical. As a result, we transitioned to **Google Colab**, which provided a cloud-based environment to run the training. However, this introduced new constraints, particularly **resource limitations** such as restricted GPU availability and execution timeouts.  \n",
    "\n",
    "To create a meaningful dataset, we leveraged **LangChain** to process documents by splitting them into manageable **text chunks**. We then generated **embeddings** using **sentence-transformers** and stored them in a **FAISS similarity index** for efficient retrieval. To generate **synthetic queries**, we selected random chunks and retrieved semantically similar ones using FAISS before passing them to a **pre-trained LLM** for query generation. Initially, we planned to further **augment these queries** using additional prompts to improve dataset diversity, but due to **time constraints**, we had to limit the augmentation process.  \n",
    "\n",
    "Once we had a sufficiently large dataset, we proceeded with training. We used the **same dataset** to fine-tune the pre-trained model for our specific task. After completing the fine-tuning process, we implemented a **Retrieval-Augmented Generation (RAG) architecture** to ensure that responses were **more relevant and concise** by dynamically retrieving relevant context during inference. However, due to **limited time and resources**, we were unable to fully integrate an **in-memory database**, which could have further optimized the retrieval process.  \n",
    "\n",
    "\n",
    "## **Conclusion**  \n",
    "This project provided **valuable insights into the end-to-end process of fine-tuning LLMs**, from **data preparation** and **synthetic query generation** to **model training** and **RAG-based retrieval**. We encountered and overcame several challenges, including **dataset generation complexities, computational resource constraints in Colab, and limited dataset size** for effective training.  \n",
    "\n",
    "Despite these challenges, we successfully **trained and deployed a task-specific model** that improved response quality through **RAG**. Future improvements could include **enhanced query augmentation**, **a larger dataset**, and **integration of an in-memory vector database** to optimize retrieval speed and accuracy. The project reinforced the importance of **efficient dataset creation, resource-aware training, and real-time information retrieval** in building effective AI systems.  \n",
    "\n",
    "---\n",
    "\n",
    "## **References**  \n",
    "1. **Synthetic Data Generation using LLMs** - [Confident AI Blog](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms)  \n",
    "2. **Qwen2.5 Fine-Tuning on Colab** - [Unsloth Colab Notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb#scrollTo=upcOlWe7A1vc)  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
