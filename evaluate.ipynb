{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model with RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Ensure the data is a list of dictionaries\n",
    "if isinstance(data, dict):  # In case the JSON data isn't a list\n",
    "    data = [data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./saved_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./saved_model\")\n",
    "\n",
    "# Set pad_token explicitly if it doesn't exist or overlaps with eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test interaction with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with the model\n",
    "def interact_with_model(prompt):\n",
    "    # Tokenize the input prompt with padding and attention_mask\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    # Ensure the attention_mask is passed to the model\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    # Decode the generated output sequence\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "# Test the model with simple prompts\n",
    "test_prompts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is the weather today?\",\n",
    "    \"Can you tell me a joke?\",\n",
    "    \"What's your favorite color?\"\n",
    "]\n",
    "\n",
    "# Display responses\n",
    "for prompt in test_prompts:\n",
    "    response = interact_with_model(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "#load_dotenv(find_dotenv())\n",
    "chunks=[]\n",
    "for item in data:\n",
    "    chunks.append(item['context'])\n",
    "# Setup vector database\n",
    "client = weaviate.Client(\n",
    "  embedded_options = EmbeddedOptions()\n",
    ")\n",
    "\n",
    "# Populate vector database\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client = client,    \n",
    "    documents = chunks,\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    by_text = False\n",
    ")\n",
    "\n",
    "# Define vectorstore as retriever to enable semantic search\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model\n",
    "    | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the RAGAS Evaluation Pipeline: RAGAS evaluates the correctness of the generated responses based on context, question, and ground truth answers. Hereâ€™s how to build the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate_results\n",
    "from ragas.metrics import precision, recall, f1_score, contextual_accuracy\n",
    "\n",
    "def qwen_generate(prompt, max_length=512, temperature=0, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generates output using Qwen-2.5-3B-Instruct model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "# Prepare the dataset for RAGAS\n",
    "dataset = [\n",
    "    {\n",
    "        \"context\": entry[\"context\"],\n",
    "        \"question\": entry[\"question\"],\n",
    "        \"ground_truth\": entry[\"answer\"],\n",
    "    }\n",
    "    for entry in data\n",
    "]\n",
    "\n",
    "# Define the retriever (Qwen model)\n",
    "def retriever(context, question):\n",
    "    \"\"\"\n",
    "    Takes the context and question, and passes it to Qwen model.\n",
    "    Returns the generated answer.\n",
    "    \"\"\"\n",
    "    # Construct the prompt\n",
    "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer the Question based on context:\"\n",
    "    return qwen_generate(prompt)\n",
    "\n",
    "# Generate predictions using your model\n",
    "for entry in dataset:\n",
    "    entry[\"predicted_answer\"] = retriever(entry[\"context\"], entry[\"question\"])\n",
    "\n",
    "# Evaluate the results using RAGAS metrics\n",
    "metrics = [precision, recall, f1_score, contextual_accuracy]\n",
    "evaluation_results = evaluate_results(dataset, metrics)\n",
    "\n",
    "# Print the evaluation\n",
    "print(evaluation_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
